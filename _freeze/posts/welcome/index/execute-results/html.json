{
  "hash": "80d6a4fe392bfd79a0970df2143f1c64",
  "result": {
    "markdown": "---\ntitle: \"Proper Web Scraping Etiquette\"\nauthor: \"Peter Boshe\"\ndate: \"2021-12-03\"\ncategories: [\"Unguided Project\", \"Data Mining\", \"Web Scraping\"]\ncode-fold: true\ncode-summary: \"Click to view sample code\"\ncode-copy: true\ntoc: true\ntoc-depth: 5\ndescription: \"In this article we take a look at how we can scrape raw information/data from websites without the hassle of copying and pasting and clicking and retyping whole data tables and *then* analysing your data.\"\n---\n\n\n## Why Web Scrape?\n\n![](thumbnail.jpg)\n\n\n\n\n\nIf you are working in today's world, Many a times you would come across tasks where you need to find data published on a website, you finally find and it's all good.. except.. where is the download button?\n\nNow, how frustrated I would usually get would depend on the amount of data to be copy&pasted or retyped, number of pages I have to click through, how many values I have to filter to find my required information, which is usually a lot.\n\nBut thanks to the data scraping tools and techniques available today, I was able to spice up my workflow by extracting the information required by selecting the data elements straight from the html page. An example of a scraping script to be demonstrated later in this article.\n\n## Web Scraping Etiquette\n\nA wise philosopher named Ben once said\n\n> \"with great power comes great responsibility\"\n\nWell this relates to a lot of things and Data Scraping is no exception.\n\nThe following are three staples for scraping politeness;\n\n1.  **Introduce yourself**\n\n    It is always a good idea to leave an introductory note in the `User-Agent` Field, the note should include your email and purpose of scraping. exaple below; <!-- #REVIEW to check alternative for r -->\n\n\n    \n\n\n2.  **Throttle your requests**\n\n    As web scraping by code does not offer the natural buffer between requests to a website like one would do when retrieving the data manually, and while servers for websites like Wikipedia would likely not feel a thing. One can easily overload a server if the frequency of requests is not supported, especially when dealing with large amounts of data. If many people are simultaneously trying to get a massive amount of data this could easily overwhelm the system which can hurt a service provider, we do not want that..\n\n3.  **Cite your source**\n\n    Well this golden rule depends on the reason of scraping, It is always Imperative that we cite our data sources in any publication.\n\nNow let me introduce my personal tenets to webscraping I have learned to abide by given my experience;\n\n## Webscraping tenets\n\n-   \n\n    Xpaths over CSS selectors\n\n    :   When web scraping you are going to want to select items of interest from the target of interest, and depending on what language you use to code, it normally comes down to whether you are going to identify the items for your script by their **CSS/HTML** tags or by their **xpaths**. From my recent endevours I have found the latter to be a more robust option as the xpaths tend to be more uniform throughout most pages.\n\n-   \n\n    Hope for the best, Plan for the worst\n\n    :   While scraping it's good practise to test out your script on the web pages of interest, we are not always sure the web format will always remain the same in the future. So it is important to have a contigency plan, for instance you would much rather have your script impute 'NA's on items it failed to scrape and continue with its scraping, instead of having your script fail and stop on its first obstacle. In R this can be achieved by functions like `safely()` and `possibly()` in the tidyverse ecosystem.\n\n-   \n\n    If your Internet is shaky, increase your timeout limit\n\n    :   I, for one, do not have very reliable internet service so my script kept getting timed out in between the requests, and when you are timed out, your script goes sit on a corner. You get an error with no results. as a solution I simply increased my timeout limit to 60 seconds and I had no further issues.\n\n-   \n\n    Functional programming FOR THE WIN!\n\n    :   This is a concept I learned of recently that eases the process of repetitive tasks.\n\nFor instance, for a task where we are supposed to scrape a parent page, extract the links of several job posts from the parent page, then exctract information from the individual pages from each of the links scraped from the parent page. Writing code to scrape each page individually will be time draining, to say the least.\n\nUsing functional programming this can be acheived as follows;\n\n1.  pick a link from the list of pages to scrape\n2.  write code to scrape the target info from that one page\n3.  if code works, turn code into a **function**\n4.  **map** the function to the whole list of url links (remember to throttle requests as explained above and to allow for failed attempts)\n5.  tabulate results\n\nBelow is sample script in R for a recent webscraping project I did recently using the principles above toobtain the data to create [this dashboard.](%22https://petertoc.github.io/flexdashoard%22) hosted on github pages.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\n# script to extract to webscrape indeed for entry level data scientist\n\n# Author: Peter Boshe\n# Version: 2022-05-07\n\n# Packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)\nlibrary(broom)\nrequire(wordcloud2)\nrequire(tm)\n\n# Parameters\nurl <- \"https://www.indeed.com/jobs?q=entry%20level%20data%20scientist&l=Remote&from=searchOnHP&vjk=e4dd563aa0c5df59\"\ndomain <- \"https://www.indeed.com\"\nfile_out <- here::here(\"data/table.rds\")\nfile_out2 <- here::here(\"data/wordcloud.html\")\nreport <- here::here(\"index.Rmd\")\nhtml_output <- here:: here(\"docs/\")\nlog_output <- here::here(\"data/logs/\")\n\n# ============================================================================\n\n# Code\n\n\n\nhtml <- url |>\n  GET(timeout(60)) |>\n  read_html()\n\n\njob_title <- html |>\n  html_nodes(\"span[title]\") |>\n  html_text()\n\n\ncompany <- html |>\n  html_nodes(\"span.companyName\") |>\n  html_text()\n\nlocation <- html |>\n  html_nodes(\"div.companyLocation\") |>\n  html_text()\n\n\nlinks <- html |>\n  html_nodes(xpath = \"/html/body//tbody//div[1]/h2/a\") |>\n  html_attr(\"href\")\n\nmax_length <- length(job_title)\ndf <- data.frame(job_title,\n                 company,\n                 location,\n                 domain,\n                 links) |>\n  mutate(url_link = str_c(domain,\"\",links)) |>\n  select(job_title, company, location, url_link)\n\n\n# second iteration through the links --------------------------------------\n\n# test with one link\n\n# x <- \"https://www.indeed.com/rc/clk?jk=655e1551430353b4&fccid=11619ce0d3c2c733&vjs=3\"\n\nextract_description <- function(x) {\n\n  Sys.sleep(2) # to pause between requests\n\n  cat(\".\") # stone age progress bar\n\n  # html2 <- read_html(x)\n  html2 <- x |>\n    GET(timeout(60)) |> # important to not get timed out in some of the requests\n    read_html()\n\n  job_description <- html2 |>\n    html_nodes(xpath = '//*[@id=\"jobDescriptionText\"]') |>\n    html_text() |>\n    str_squish()\n\n  count_r <- job_description |>\n    str_count('[./ ,]R{1}[./ ,]')\n\n  r_present <- job_description |>\n    str_detect('[./ ,]R{1}[./ ,]')\n\n\n  data.frame(job_description = job_description,\n         r_present = r_present,\n         count_r = count_r)    #important to name the variables to avoid script failure\n\n}\n\n\n# functional programming for the win! -------------------------------------\n\n\nlisted_df <- df |>\n  mutate(description = map(url_link, safely(~ extract_description(.x), otherwise = NA_character_)))\n\n\n# Our new data set --------------------------------------------------------\n\nindeed_df <- listed_df |>\n  unnest(description) |>\n  unnest(description) |>\n  arrange(desc(count_r))\n\n# Write out table\n\nwrite_rds(indeed_df,file_out)\n\n\n# word cloud --------------------------------------------------------------\n\n\n\ndf <- indeed_df\n\n# text mining\n# create corpus function\ncorpus_tm <- function(x){\n  corpus_tm <- Corpus(VectorSource(x))\n}\n#create corpus\ndf |>\n  pull(job_description) |>\n  unlist() |> # might need to remove\n  corpus_tm() ->corpus_descriptions\n\n#inspect corpus\n# summary(corpus_descriptions)\n\ncorpus_descriptions |>\n  tm_map(removePunctuation) |>\n  tm_map(stripWhitespace) |>\n  tm_map(content_transformer(function(x) iconv(x, to='UTF-8', sub='byte'))) |>\n  tm_map(removeNumbers) |>\n  tm_map(removeWords, stopwords(\"en\")) |>\n  tm_map(content_transformer(tolower)) |>\n  tm_map(removeWords, c(\"etc\",\"ie\",\"eg\", stopwords(\"english\"))) -> clean_corpus_descriptions\n\n# inspect content\n\n#clean_corpus_descriptions[[1]]$content\n\n# create termdocumentmatrix to attain frequent terms\n\nfind_freq_terms_fun <- function(corpus_in){\n  doc_term_mat <- TermDocumentMatrix(corpus_in)\n  freq_terms <- findFreqTerms(doc_term_mat)[1:max(doc_term_mat$nrow)]\n  terms_grouped <- doc_term_mat[freq_terms,] %>%\n    as.matrix() %>%\n    rowSums() %>%\n    data.frame(Term=freq_terms, Frequency = .) %>%\n    arrange(desc(Frequency)) %>%\n    mutate(prop_term_to_total_terms=Frequency/nrow(.))\n  return(data.frame(terms_grouped))\n}\n\ndescription_freq_terms <- data.frame(find_freq_terms_fun(clean_corpus_descriptions))\n\n# save out wordcloud\n\nhtmlwidgets::saveWidget(wordcloud2::wordcloud2(description_freq_terms[,1:2], shape=\"circle\",\n                                               size=1.6, color='random-light', backgroundColor=\"#7D1854\"),  #ED581F\n                        file = file_out2,\n                        selfcontained = FALSE)\n\n# knit report\n\nrmarkdown::render(report, output_dir = html_output)\n\n\n# clean environment\nrm(list = ls())\n\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}