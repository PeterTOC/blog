[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Boshe | Data Specialist",
    "section": "",
    "text": "Deploying an Interactive Quarto Report\n\n\n\n\n\nThis article is meant to be a step by step guide in publishing an interactive analytical report on the web for your stake-holders\n\n\n\n\n\n\nNov 29, 2022\n\n\nPeter Boshe\n\n\n1 min\n\n\nAfyaIntelligence\n\n\nguide\n\n\n\n\n\n\n  \n\n\n\n\nProper Web Scraping Etiquette\n\n\n\n\n\nIn this article we take a look at how we can scrape raw information/data from websites without the hassle of copying and pasting and clicking and retyping whole data tables and then analysing your data.\n\n\n\n\n\n\nDec 3, 2021\n\n\nPeter Boshe\n\n\n7 min\n\n\nlearning\n\n\n\n\n\n\n  \n\n\n\n\nTanzania Mainland Football Championships; 1965 - 2020\n\n\n\n\n\nThis is a racing bar chart of the male football championships and the respective teams that have won over the past half century\n\n\n\n\n\n\nNov 29, 2021\n\n\nPeter Boshe\n\n\n3 min\n\n\nportfolio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Tanzania Mainland Football Championships; 1965 - 2020",
    "section": "",
    "text": "This is a racing bar chart of the championships and the respective teams that have won over the past half century(acc. to Data),\nWould you have guessed that less than 10 teams (only 9 teams) have won since the conception of the cup?\n\n\nCode\n## setting up the environment\n# clear environment \nrm(list = ls())\n\n# setting environment \nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(lubridate)\nlibrary(gifski)\nlibrary(png)\nlibrary(knitr)\n\n# loading data \ndf <- read_excel(\"Tanzania mainland championships.xlsx\")\n\n## pre-processing\ndf2 <- df %>% \n  mutate(Cup = 1) %>% \n  complete(Team, Year) %>% \n  replace_na(list(Cup = 0)) %>%  \n  group_by(Team) %>% \n  mutate(count = cumsum(Cup))\n  \ndf2 %>% \n  group_by(Year) %>% \n  arrange(Year, -count) %>% \n  #assign ranking\n  mutate(rank = 1:n()) ->\nranked_by_year\n\nmy_theme <- theme_classic(base_size = 22, \n                          base_family = \"Times\") +\n  theme(axis.text.y = element_blank()) +\n  theme(axis.ticks.y = element_blank()) +\n  theme(axis.line.y = element_blank()) +\n  theme(legend.position = \"none\")\n  # theme(legend.background = element_rect(fill = \"gainsboro\")) +\n  # theme(plot.background = element_rect(fill = \"gainsboro\")) +\n  # theme(panel.background = element_rect(fill = \"gainsboro\"))\n\n### Static Plot\nvalues <- c(\"Azam FC\" = \"blue\", \"Young Africans SC\" = \"yellow\", \"Simba SC\" = \"red\", \"Hybrid SC\" = \"brown\", \"Cosmopolitan\" = \"lightblue\", \"Coastal Union\" = \"black\", \"Tukuyu Stars\" = \"orange\", \"Mtibwa Sugar\" = \"darkgreen\")\n\n\nranked_by_year %>% \n   ggplot(\n  aes(xmin = 0,\n      xmax = count,\n      ymin = rank - .45,\n      ymax = rank + .45,\n      y = rank,\n    fill = Team\n  )\n) +\n  geom_rect(alpha = .7) +  \n  geom_text(col = \"gray13\",  \n            hjust = \"right\",  \n            aes(label = Team),  \n            x = -1) + \n    geom_text(aes(count + 1,\n                  label = round(count)),\n            color = \"black\",\n            position = \"identity\",\n            size = 3.5) +\n  scale_y_reverse() +\n  scale_fill_manual(\"legend\", values = values) +\n  # scale_x_continuous(\n  #   limits = c(-39, 27),\n  #   breaks = 1:30) +\n  labs(\n    title = \"Mainland Championships\",\n    x = \"Cups Won\",\n    y = \"\"\n  ) + \n    facet_wrap(~ Year) +\n  my_theme ->\nmy_plot\n\n\n## Make Gif\nmy_plot +  \n  facet_null() +  \n  scale_x_continuous(\n    limits = c(-10, 40),\n    breaks = seq(0, 30, 5)) +\n  geom_text(x = 30, y = -7.5,  \n            family = \"Times\",  \n            aes(label = as.character(Year)),  \n            size = 20, col = \"grey18\") +  \n  aes(group = Team) +  \n  transition_time(Year) -> p\n\n\n\nanimate(p, fps = 2, duration = 28, width = 800, height = 600) -> anim\n#save the gif \nanim_save(\"../../assets/mainland_championship.gif\", anim)\n# display the gif\nanim\n\n\n\n\n\nracing plot of the mainland champions in Tanzania"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Proper Web Scraping Etiquette",
    "section": "",
    "text": "If you are working in today‚Äôs world, Many a times you would come across tasks where you need to find data published on a website, you finally find and it‚Äôs all good.. except.. where is the download button?\nNow, how frustrated I would usually get would depend on the amount of data to be copy&pasted or retyped, number of pages I have to click through, how many values I have to filter to find my required information, which is usually a lot.\nBut thanks to the data scraping tools and techniques available today, I was able to spice up my workflow by extracting the information required by selecting the data elements straight from the html page. An example of a scraping script to be demonstrated later in this article."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I enjoy tackling real world\nproblems with data analysis and\nmachine learning. I specialize in building\ndata products, reports and facilitating\ndata-driven decisions using R.\nSo I made this blog to be able to share what I find interesting.\n\n\n\nI am a well disciplined and enthusiastic data analyst with a just as strong background in project management and structural engineering.\nI enjoy wrangling data from its rawest forms to actionable insight, models and visuals that are more comprehensible to us, humans. All in the hopes of highlighting the underlying mechanisms of what causes apparent trends in our data.\nTo that effect I find myself fond of topics that involve cluster analysis, network analysis, and inferential analysis that focus on causality. Would you look at that‚Ä¶ I have a bias.\nWhen I am not up-skilling or earning a buck, I enjoy playing pc games (currently obsessed with splitgate ), having a laugh, sci-fi, anime and long walks by the beach. In no particular order.\n\n\n\n\nTelematic/IOT Analytics\nBusiness Data Analysis\nSports Analytics\nHealthcare Analytics\n\n\n\n\n\nStatistical Inference\nPractical/Applied Machine Learning (read. building ML pipelines)\nSocial Network Analysis\nAWS infrastructure\nJavascript(D3) for web interactive data visuals\n\n\n\n\n\n\n\nemail: peterboshe@gmail.com\nlinkedin: www.linkedin.com/in/peterboshe\nweb page: https://peterboshe.netlify.app/\nmobile: +255 688 372 127 / +255 769 405 758"
  },
  {
    "objectID": "posts/welcome/index.html#why-web-scrape",
    "href": "posts/welcome/index.html#why-web-scrape",
    "title": "Proper Web Scraping Etiquette",
    "section": "Why Web Scrape?",
    "text": "Why Web Scrape?\nIf you are working in today‚Äôs world, Many a times you would come across tasks where you need to find data published on a website, you finally find and it‚Äôs all good.. except.. where is the download button?\nNow, how frustrated I would usually get would depend on the amount of data to be copy&pasted or retyped, number of pages I have to click through, how many values I have to filter to find my required information, which is usually a lot.\nBut thanks to the data scraping tools and techniques available today, I was able to spice up my workflow by extracting the information required by selecting the data elements straight from the html page. An example of a scraping script to be demonstrated later in this article."
  },
  {
    "objectID": "posts/welcome/index.html#web-scraping-etiquette",
    "href": "posts/welcome/index.html#web-scraping-etiquette",
    "title": "Proper Web Scraping Etiquette",
    "section": "Web Scraping Etiquette",
    "text": "Web Scraping Etiquette\nA wise philosopher named Ben once said\n\n‚Äúwith great power comes great responsibility‚Äù\n\nWell this relates to a lot of things and Data Scraping is no exception.\nThe following are three staples for scraping politeness;\n\nIntroduce yourself\nIt is always a good idea to leave an introductory note in the User-Agent Field, the note should include your email and purpose of scraping. exaple below; \nThrottle your requests\nAs web scraping by code does not offer the natural buffer between requests to a website like one would do when retrieving the data manually, and while servers for websites like Wikipedia would likely not feel a thing. One can easily overload a server if the frequency of requests is not supported, especially when dealing with large amounts of data. If many people are simultaneously trying to get a massive amount of data this could easily overwhelm the system which can hurt a service provider, we do not want that..\nCite your source\nWell this golden rule depends on the reason of scraping, It is always Imperative that we cite our data sources in any publication.\n\nNow let me introduce my personal tenets to webscraping I have learned to abide by given my experience;"
  },
  {
    "objectID": "posts/welcome/index.html#webscraping-tenets",
    "href": "posts/welcome/index.html#webscraping-tenets",
    "title": "Proper Web Scraping Etiquette",
    "section": "Webscraping tenets",
    "text": "Webscraping tenets\n\n\nXpaths over CSS selectors\n\nWhen web scraping you are going to want to select items of interest from the target of interest, and depending on what language you use to code, it normally comes down to whether you are going to identify the items for your script by their CSS/HTML tags or by their xpaths. From my recent endevours I have found the latter to be a more robust option as the xpaths tend to be more uniform throughout most pages.\n\n\n\nHope for the best, Plan for the worst\n\nWhile scraping it‚Äôs good practise to test out your script on the web pages of interest, we are not always sure the web format will always remain the same in the future. So it is important to have a contigency plan, for instance you would much rather have your script impute ‚ÄôNA‚Äôs on items it failed to scrape and continue with its scraping, instead of having your script fail and stop on its first obstacle. In R this can be achieved by functions like safely() and possibly() in the tidyverse ecosystem.\n\n\n\nIf your Internet is shaky, increase your timeout limit\n\nI, for one, do not have very reliable internet service so my script kept getting timed out in between the requests, and when you are timed out, your script goes sit on a corner. You get an error with no results. as a solution I simply increased my timeout limit to 60 seconds and I had no further issues.\n\n\n\nFunctional programming FOR THE WIN!\n\nThis is a concept I learned of recently that eases the process of repetitive tasks.\n\n\n\nFor instance, for a task where we are supposed to scrape a parent page, extract the links of several job posts from the parent page, then exctract information from the individual pages from each of the links scraped from the parent page. Writing code to scrape each page individually will be time draining, to say the least.\nUsing functional programming this can be acheived as follows;\n\npick a link from the list of pages to scrape\nwrite code to scrape the target info from that one page\nif code works, turn code into a function\nmap the function to the whole list of url links (remember to throttle requests as explained above and to allow for failed attempts)\ntabulate results\n\nBelow is sample script in R for a recent webscraping project I did recently using the principles above toobtain the data to create this dashboard. hosted on github pages.\n\n\nClick to view sample code\n\n# script to extract to webscrape indeed for entry level data scientist\n\n# Author: Peter Boshe\n# Version: 2022-05-07\n\n# Packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)\nlibrary(broom)\nrequire(wordcloud2)\nrequire(tm)\n\n# Parameters\nurl <- \"https://www.indeed.com/jobs?q=entry%20level%20data%20scientist&l=Remote&from=searchOnHP&vjk=e4dd563aa0c5df59\"\ndomain <- \"https://www.indeed.com\"\nfile_out <- here::here(\"data/table.rds\")\nfile_out2 <- here::here(\"data/wordcloud.html\")\nreport <- here::here(\"index.Rmd\")\nhtml_output <- here:: here(\"docs/\")\nlog_output <- here::here(\"data/logs/\")\n\n# ============================================================================\n\n# Code\n\n\n\nhtml <- url |>\n  GET(timeout(60)) |>\n  read_html()\n\n\njob_title <- html |>\n  html_nodes(\"span[title]\") |>\n  html_text()\n\n\ncompany <- html |>\n  html_nodes(\"span.companyName\") |>\n  html_text()\n\nlocation <- html |>\n  html_nodes(\"div.companyLocation\") |>\n  html_text()\n\n\nlinks <- html |>\n  html_nodes(xpath = \"/html/body//tbody//div[1]/h2/a\") |>\n  html_attr(\"href\")\n\nmax_length <- length(job_title)\ndf <- data.frame(job_title,\n                 company,\n                 location,\n                 domain,\n                 links) |>\n  mutate(url_link = str_c(domain,\"\",links)) |>\n  select(job_title, company, location, url_link)\n\n\n# second iteration through the links --------------------------------------\n\n# test with one link\n\n# x <- \"https://www.indeed.com/rc/clk?jk=655e1551430353b4&fccid=11619ce0d3c2c733&vjs=3\"\n\nextract_description <- function(x) {\n\n  Sys.sleep(2) # to pause between requests\n\n  cat(\".\") # stone age progress bar\n\n  # html2 <- read_html(x)\n  html2 <- x |>\n    GET(timeout(60)) |> # important to not get timed out in some of the requests\n    read_html()\n\n  job_description <- html2 |>\n    html_nodes(xpath = '//*[@id=\"jobDescriptionText\"]') |>\n    html_text() |>\n    str_squish()\n\n  count_r <- job_description |>\n    str_count('[./ ,]R{1}[./ ,]')\n\n  r_present <- job_description |>\n    str_detect('[./ ,]R{1}[./ ,]')\n\n\n  data.frame(job_description = job_description,\n         r_present = r_present,\n         count_r = count_r)    #important to name the variables to avoid script failure\n\n}\n\n\n# functional programming for the win! -------------------------------------\n\n\nlisted_df <- df |>\n  mutate(description = map(url_link, safely(~ extract_description(.x), otherwise = NA_character_)))\n\n\n# Our new data set --------------------------------------------------------\n\nindeed_df <- listed_df |>\n  unnest(description) |>\n  unnest(description) |>\n  arrange(desc(count_r))\n\n# Write out table\n\nwrite_rds(indeed_df,file_out)\n\n\n# word cloud --------------------------------------------------------------\n\n\n\ndf <- indeed_df\n\n# text mining\n# create corpus function\ncorpus_tm <- function(x){\n  corpus_tm <- Corpus(VectorSource(x))\n}\n#create corpus\ndf |>\n  pull(job_description) |>\n  unlist() |> # might need to remove\n  corpus_tm() ->corpus_descriptions\n\n#inspect corpus\n# summary(corpus_descriptions)\n\ncorpus_descriptions |>\n  tm_map(removePunctuation) |>\n  tm_map(stripWhitespace) |>\n  tm_map(content_transformer(function(x) iconv(x, to='UTF-8', sub='byte'))) |>\n  tm_map(removeNumbers) |>\n  tm_map(removeWords, stopwords(\"en\")) |>\n  tm_map(content_transformer(tolower)) |>\n  tm_map(removeWords, c(\"etc\",\"ie\",\"eg\", stopwords(\"english\"))) -> clean_corpus_descriptions\n\n# inspect content\n\n#clean_corpus_descriptions[[1]]$content\n\n# create termdocumentmatrix to attain frequent terms\n\nfind_freq_terms_fun <- function(corpus_in){\n  doc_term_mat <- TermDocumentMatrix(corpus_in)\n  freq_terms <- findFreqTerms(doc_term_mat)[1:max(doc_term_mat$nrow)]\n  terms_grouped <- doc_term_mat[freq_terms,] %>%\n    as.matrix() %>%\n    rowSums() %>%\n    data.frame(Term=freq_terms, Frequency = .) %>%\n    arrange(desc(Frequency)) %>%\n    mutate(prop_term_to_total_terms=Frequency/nrow(.))\n  return(data.frame(terms_grouped))\n}\n\ndescription_freq_terms <- data.frame(find_freq_terms_fun(clean_corpus_descriptions))\n\n# save out wordcloud\n\nhtmlwidgets::saveWidget(wordcloud2::wordcloud2(description_freq_terms[,1:2], shape=\"circle\",\n                                               size=1.6, color='random-light', backgroundColor=\"#7D1854\"),  #ED581F\n                        file = file_out2,\n                        selfcontained = FALSE)\n\n# knit report\n\nrmarkdown::render(report, output_dir = html_output)\n\n\n# clean environment\nrm(list = ls())"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About me",
    "text": "About me\nI am a well disciplined and enthusiastic data analyst with a just as strong background in project management and structural engineering.\nI enjoy wrangling data from its rawest forms to actionable insight, models and visuals that are more comprehensible to us, humans. All in the hopes of highlighting the underlying mechanisms of what causes apparent trends in our data.\nTo that effect I find myself fond of topics that involve cluster analysis, network analysis, and inferential analysis that focus on causality. Would you look at that‚Ä¶ I have a bias.\nWhen I am not up-skilling or earning a buck, I enjoy playing pc games (currently obsessed with splitgate ), having a laugh, sci-fi, anime and long walks by the beach. In no particular order."
  },
  {
    "objectID": "about.html#fields-explored",
    "href": "about.html#fields-explored",
    "title": "About",
    "section": "üî≠ Fields Explored;",
    "text": "üî≠ Fields Explored;\n\nTelematic/IOT Analytics\nBusiness Data Analysis\nSports Analytics\nHealthcare Analytics"
  },
  {
    "objectID": "about.html#im-currently-learning-more-on",
    "href": "about.html#im-currently-learning-more-on",
    "title": "About",
    "section": "üå± I'm currently learning more on;",
    "text": "üå± I'm currently learning more on;\n\nStatistical Inference\nPractical/Applied Machine Learning (read. building ML pipelines)\nSocial Network Analysis\nAWS infrastructure\nJavascript(D3) for web interactive data visuals"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About",
    "section": "",
    "text": "üì´ Contact Details\n\nemail: peterboshe@gmail.com\nlinkedin: www.linkedin.com/in/peterboshe\nweb page: https://peterboshe.netlify.app/\nmobile: +255 688 372 127 / +255 769 405 758"
  },
  {
    "objectID": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html",
    "href": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html",
    "title": "Deploying an Interactive Quarto Report",
    "section": "",
    "text": "Most of the analytical work usually ends up as reports, so streamlining your workflow between analytics and sharing your report is pivotal in any data profession.\nI recently came upon Quarto , an open-source language agnostic reporting tool that, I believe, should be in the back pocket of any data professional as it doesn‚Äôt matter the scripting language of choice, your reports will be amazing.\nIn this article however I will be using;\n\nUbuntu 22.04\nR &Rstudio\nQuarto\nServer hosted by DigitalOcean"
  },
  {
    "objectID": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html#pre-requisites",
    "href": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html#pre-requisites",
    "title": "Deploying an Interactive Quarto Report",
    "section": "Pre-requisites",
    "text": "Pre-requisites"
  },
  {
    "objectID": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html#procedures",
    "href": "posts/Publishing interactive documents with quarto/publishing_with_quarto.html#procedures",
    "title": "Deploying an Interactive Quarto Report",
    "section": "Procedures",
    "text": "Procedures\n\nInstall Quarto\n\n\nLaunch Rstudio\nAgain you can use any text editor of choice as quarto has support for ‚Ä¶‚Ä¶.., I will however be using Rstudio for my project management.\n\nCreate new project\nIf you haven‚Äôt already, Install the quarto package‚Ä¶.\nOnce installed you will have access to the features on your Rstudio Editor\nClick on File > New Project > New Directory > Quarto Project > create a name for your new project & specify your file directory as follows;\n\n\n\n\n\n\n\nIt is always good practise to use version control and environments in projects\n\n\n\n\n\n\nIf all went well you should now have your template quarto document launched in your new project.\nAll you need to do is click on render and see your first output\n\nWe can edit the yaml header (the first three lines in previous image) to for our specific report.\n\nMuch better, now that we have confirmed that works, we can do our analysis.\nI am going to work with data from AfyaIntelligence to build my interactive dashboard.\n\n\n\nAnalysis\n\nImport your dataset"
  }
]